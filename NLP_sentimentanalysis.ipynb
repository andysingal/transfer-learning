{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "NLP-sentimentanalysis.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyNwWpzFjxQzaa3oIhSuJYAQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/andysingal/transfer-learning/blob/main/NLP_sentimentanalysis.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VhGRKuSCSlC_"
      },
      "source": [
        "!nvidia-smi\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OYa6PVLNTepI"
      },
      "source": [
        "from __future__ import absolute_import,division,print_function,unicode_literals\n",
        "try:\n",
        "  !pip uninstall tb-nightly tensorboardX tensorboard\n",
        "  !pip install tf-nightly\n",
        "except Exception:\n",
        "     pass\n",
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "import os\n",
        "import datetime\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "%load_ext tensorboard\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJVNW1HrVu-s"
      },
      "source": [
        "import pkg_resources\n",
        "for entry_point in pkg_resources.iter_entry_points('tensorflow_plugins'):\n",
        "  print(entry_point.dist)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wM9k-kWgWq7f"
      },
      "source": [
        "#!ls -alrt /usr/local/lib/python3.7/dist-packages/tensorboard*\n",
        "!rm -r  /usr/local/lib/python3.7/dist-packages/tensorboardcolab-0.22.dist-info\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tatQY9jLYAuh"
      },
      "source": [
        "print(tf.__version__)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "33j-yDfeYHoF"
      },
      "source": [
        "tf.config.experimental.list_physical_devices()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9xstdeECYHrS"
      },
      "source": [
        "dataset,info = tfds.load('amazon_us_reviews/Personal_Care_Appliances_v1_00', with_info=True)\n",
        "train_dataset = dataset['train']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cmvGz7tPYHuW"
      },
      "source": [
        "info"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bj4pXBz9YH0j"
      },
      "source": [
        "BUFFER_SIZE=30000\n",
        "BATCH_SIZE=128"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "STML23b9s56W"
      },
      "source": [
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE,reshuffle_each_iteration=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2hWEZGKwgmP"
      },
      "source": [
        "for reviews in train_dataset.take(2):\n",
        "  print(reviews)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MgPlSMwhw7lZ"
      },
      "source": [
        "for reviews in train_dataset.take(10):\n",
        "   review_text = reviews['data']\n",
        "   print(review_text.get('review_body').numpy())\n",
        "   print(review_text.get('star_rating'))\n",
        "   print(tf.where(review_text.get('star_rating') >3,1,0).numpy())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nq_ILTSF0sT5"
      },
      "source": [
        "Building a vocabulary, tokenising and encoding.\n",
        "First, build a vocabulary by tokenizing the text into a collection of individual unique words. There are a few ways to do this in both TensorFlow and Python. For this tutorial:\n",
        "Iterate over each exampleâ€™s numpy value.\n",
        "Use tfds.features.text.Tokenizer to split it into tokens.\n",
        "Collect these tokens into a Python set, to remove duplicates.\n",
        "Get the size of the vocabulary for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "swbnQDImyTvc"
      },
      "source": [
        "tokenizer = tfds.deprecated.text.Tokenizer()\n",
        "\n",
        "vocabulary_set = set()\n",
        "for _,reviews in train_dataset.enumerate():\n",
        "  review_text = reviews['data']\n",
        "  review_tokens = tokenizer.tokenize(review_text.get('review_body').numpy())\n",
        "  vocabulary_set.update(review_tokens)\n",
        "\n",
        "vocab_size = len(vocabulary_set)\n",
        "vocab_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCcK08Sb1I-J"
      },
      "source": [
        "Encode examples\n",
        "Create an encoder by passing the vocabulary_set to tfds.features.text.TokenTextEncoder. The encoder's encode method takes in a string of text and returns a list of integers.\n",
        "\n",
        "https://alexmoltzau.medium.com/building-a-text-dataset-c5c1481395f4\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "34BFXcTh1RpE"
      },
      "source": [
        "encoder = tfds.deprecated.text.TokenTextEncoder(vocabulary_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "07PdFqBS1d0q"
      },
      "source": [
        "#print(vocabulary_set)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qpu90FsY2B8g"
      },
      "source": [
        "for reviews in train_dataset.take(10):\n",
        "   review_text = reviews['data']\n",
        "   print(review_text.get('review_body').numpy())\n",
        "   encode_example = encoder.encode(review_text.get('review_body').numpy())\n",
        "   print(encode_example)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "deUUBdk62B_d"
      },
      "source": [
        "for index in encode_example:\n",
        "  print('{} ----> {}'.format(index,encoder.decode([index])))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XCEat6rD2CC0"
      },
      "source": [
        "def encode(text_sensor,label):\n",
        "  encoded_text = encoder.encode(text_sensor.numpy())\n",
        "  label = tf.where(label >3,1,0)\n",
        "  return encoded_text,label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V_YQf6go2CF_"
      },
      "source": [
        "def encode_map_fn(tensor):\n",
        "  text = tensor['data'].get('review_body')\n",
        "  label = tensor['data'].get('star_rating')\n",
        "  # py_func doesn't set the shape of the returned tensors.\n",
        "  encoded_text, label = tf.py_function(encode, \n",
        "                                       inp=[text, label], \n",
        "                                       Tout=(tf.int64, tf.int32))\n",
        "\n",
        "  # `tf.data.Datasets` work best if all components have a shape set\n",
        "  #  so set the shapes manually: \n",
        "  encoded_text.set_shape([None])\n",
        "  label.set_shape([])\n",
        "\n",
        "  return encoded_text, label"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1J8kcSwh58xW"
      },
      "source": [
        "https://notebook.community/tensorflow/docs/site/en/tutorials/load_data/text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IlwgB2ib2CI8"
      },
      "source": [
        "all_encoded_data = train_dataset.map(encode_map_fn)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wLu31eqW2CMh"
      },
      "source": [
        "for f0,f1 in all_encoded_data.take(2):\n",
        "  print(f0)\n",
        "  print(f1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQ3DNYeB-h8U"
      },
      "source": [
        "https://notebook.community/tensorflow/docs/site/en/tutorials/load_data/text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fUJ2Khu02CP_"
      },
      "source": [
        "TAKE_SIZE = 10000\n",
        "train_data = all_encoded_data.skip(TAKE_SIZE).shuffle(BUFFER_SIZE)\n",
        "train_data = train_data.padded_batch(BATCH_SIZE)\n",
        "\n",
        "test_data = all_encoded_data.take(TAKE_SIZE)\n",
        "test_data = test_data.padded_batch(BATCH_SIZE)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gzkfjp7N-4Op"
      },
      "source": [
        "Since we have introduced a new token encoding (the zero used for padding), the vocabulary size has increased by one."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FgOS9Au-5JA"
      },
      "source": [
        "vocab_size += 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5QTJ11NT_KIH"
      },
      "source": [
        "BUILDING THE MODEL"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pWnnwkMmbKxD"
      },
      "source": [
        "model = tf.keras.Sequential()\n",
        "model.add(tf.keras.layers.Embedding(vocab_size, 128))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(128,return_sequences=True)))\n",
        "model.add(tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)))\n",
        "for units in [64, 64]:\n",
        "  model.add(tf.keras.layers.Dense(units, activation='relu'))\n",
        "\n",
        "# Output layer. The first argument is the number of labels.\n",
        "model.add(tf.keras.layers.Dense(1))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXFNPFQ7bK1J"
      },
      "source": [
        "!rm -r /tmp/logs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6v12I0fFbK5N"
      },
      "source": [
        "logdir = os.path.join(\"/tmp/logs\",datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(logdir)\n",
        "checkpointer = tf.keras.callbacks.ModelCheckpoint(filepath='/tmp/sentiment_analysis.hdf5',verbose=1,save_weights_only=True,\n",
        "   # Save weights, every epoch.\n",
        "   save_freq='epoch')\n",
        "\n",
        "model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yUgRDDQfIWOq",
        "outputId": "cfdb099a-bf0c-4e25-fd64-b45e6cb13427"
      },
      "source": [
        "history = model.fit(train_data, epochs=3, validation_data=test_data, callbacks=[tensorboard_callback,checkpointer])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/3\n",
            "242/594 [===========>..................] - ETA: 42:40 - loss: 0.4089 - accuracy: 0.8038"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1laTFGikJ9eN"
      },
      "source": [
        "model.save('/tmp/final_sentiment_analysis.hdf5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzGq5om3J__b"
      },
      "source": [
        "!ls -lart /tmp/*.hdf5"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nh4ONrqaKyBU"
      },
      "source": [
        "eval_loss, eval_acc = model.evaluate(test_data)\n",
        "\n",
        "print('\\nEval loss: {:.3f}, Eval accuracy: {:.3f}'.format(eval_loss, eval_acc))"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}